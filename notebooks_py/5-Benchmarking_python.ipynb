{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"top\" id=\"top\"></a>\n",
    "\n",
    "<div align=\"center\">\n",
    "    <h1>Benchmarking</h1>\n",
    "    <a href=\"https://github.com/bernalde\">David E. Bernal Neira</a>\n",
    "    <br>\n",
    "    <i>Davidson School of Chemical Engineering, Purdue University</i>\n",
    "    <br>\n",
    "    <i>Universities Space Research Association</i>\n",
    "    <br>\n",
    "    <i>NASA QuAIL</i>\n",
    "    <br>\n",
    "    <br>\n",
    "    <a href=\"https://github.com/pedromxavier\">Pedro Maciel Xavier</a>\n",
    "    <br>\n",
    "    <i>Davidson School of Chemical Engineering, Purdue University</i>\n",
    "    <br>\n",
    "    <i>Computer Science &amp; Systems Engineering Program, Federal University of Rio de Janeiro</i>\n",
    "    <br>\n",
    "    <i>PSR Energy Consulting &amp; Analytics</i>\n",
    "    <br>\n",
    "    <br>\n",
    "    <a href=\"https://github.com/murraybj\">Benjamin J. L. Murray</a>\n",
    "    <br>\n",
    "    <i>Davidson School of Chemical Engineering, Purdue University</i>\n",
    "    <br>\n",
    "    <i>Undergraduate Research Assistant</i>\n",
    "    <br>\n",
    "    <br>\n",
    "    <a href=\"https://colab.research.google.com/github/SECQUOIA/QUBONotebooks/blob/main/notebooks_py/5-Benchmarking_python.ipynb\" target=\"_parent\">\n",
    "        <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "    </a>\n",
    "    <a href=\"#installation\">\n",
    "        <img src=\"https://img.shields.io/badge/⚙️-Installation_Instructions-blue\" alt=\"Installation Instructions\"/>\n",
    "    </a>\n",
    "    <a href=\"https://bernalde.github.io/\">\n",
    "        <img src=\"https://img.shields.io/badge/⚗️-Bernal_Lab-blue\" alt=\"Bernal Lab\"/>\n",
    "    </a>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About this Notebook\n",
    "\n",
    "With the new availability of unconventional hardware, novel algorithms, and increasingly optimized software to address optimization problems; the first question that arises is, which one is better?\n",
    "We will call the combination of hardware, algorithm, and software within a solution method a solver.\n",
    "This question is also relevant when evaluating a single solver, given that usually they rely on hyperparameters, for which the quesiton now becomes, which is the best parameter setting for a given solver?\n",
    "These questions obviously depend on the problem that one is trying to solve. The solution of the problem also depends on the budget of resources that one has available.\n",
    "\n",
    "In the case that the available resources are relatively \"unlimited\" and that the problem to solve is known, one could exhaustively try all the parameter settings within a delimited range for that instance and choose which one is the best.\n",
    "This case is idealistic, in the sense that usually one does not know a-priori which problem is there to solve (and if while testing all the parameters you solve it, what would be the point of identifying the best parameters?), and that there exists limitations in terms of resources, e.g., time, memory, or energy, when trying to address these problems.\n",
    "A case closer to reality is where you have the chance of solving problems that look similar to the one that you are interested in solving later, either because you have previously generated problems or you have identified a feature that characterizes your problem of interest and can generate random instances, which we will call as a family of instances.\n",
    "Then, you can use a larger amount of resources to solve that family of problems \"off-line\", meaning that you spend extra resources to address the problems in your family of instances although it is unrelated to the actual application.\n",
    "Finally, you would like to use the results that you found off-line as a guidance to solve your unknown problem more efficiently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example\n",
    "For illustration purposes, we will use an example that you are already familiar with, which is an Ising model. As a solver, we will use a simulated annealing code provided by D-Wave Ocean Tools."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ising model\n",
    "In order to implement the different Ising Models we will use D-Wave's packages **[dimod](https://github.com/dwavesystems/dimod)** and **[neal](https://github.com/dwavesystems/dwave-neal)**, for defining the Ising model and solving it with simulated annealing, respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem statement\n",
    "\n",
    "We pose the Ising problem as the following optimization problem:\n",
    "\n",
    "$$\n",
    "\\min_{s \\in \\{ \\pm 1 \\}^n} H(s) = \\min_{s \\in \\{ \\pm 1 \\}^n} \\sum_{(i, j) \\in E(G)} J_{i,j}s_is_j + \\sum_{i \\in V(G)} h_is_i + \\beta\n",
    "$$\n",
    "\n",
    "where we optimize over spins $s \\in \\{ \\pm 1 \\}^n$, on a constrained graph $G(V,E)$, where the quadratic coefficients are $J_{i,j}$ and the linear coefficients are $h_i$.\n",
    "We also include an arbitrary offset of the Ising model $\\beta$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example\n",
    "\n",
    "Suppose we have an Ising model defined from\n",
    "\n",
    "$$\n",
    "h = \\begin{bmatrix}\n",
    "145.0 \\\\ 122.0 \\\\ 122.0 \\\\ 266.0 \\\\ 266.0 \\\\ 266.0 \\\\ 242.5 \\\\ 266.0 \\\\ 386.5 \\\\ 387.0 \\\\ 386.5\n",
    "\\end{bmatrix},\n",
    "J = \\begin{bmatrix}\n",
    "0 & 0 & 0 & 24 & 24 & 24 & 24 & 24 & 24 & 24 & 24\\\\\n",
    "0 & 0 & 0 & 24 & 0 & 24 & 24 & 24 & 24 & 24 & 24\\\\\n",
    "0 & 0 & 0 & 0 & 24 & 0 & 24 & 24 & 24 & 24 & 24\\\\\n",
    "0 & 0 & 0 & 0 & 24 & 48 & 24 & 24 & 48 & 48 & 48\\\\\n",
    "0 & 0 & 0 & 0 & 0 & 24 & 24 & 48 & 48 & 48 & 48\\\\\n",
    "0 & 0 & 0 & 0 & 0 & 0 & 24 & 24 & 48 & 48 & 48\\\\\n",
    "0 & 0 & 0 & 0 & 0 & 0 & 0 & 24 & 48 & 48 & 48\\\\\n",
    "0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 48 & 48 & 48\\\\\n",
    "0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 72 & 72\\\\\n",
    "0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 72\\\\\n",
    "0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\\\\n",
    "\\end{bmatrix} \\text{ and }\n",
    "c_I = 1319.5\n",
    "$$\n",
    "Let's solve this problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If using this on Google collab, we need to install the packages\n",
    "try:\n",
    "  import google.colab\n",
    "  IN_COLAB = True\n",
    "except:\n",
    "  IN_COLAB = False\n",
    "\n",
    "# Let's install with dimod and neal\n",
    "if IN_COLAB:\n",
    "    !pip install -q pyomo\n",
    "    !pip install dimod\n",
    "    !pip install dwave-neal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Pyomo library, which can be installed via pip, conda or from Github https://github.com/Pyomo/pyomo\n",
    "import pyomo.environ as pyo\n",
    "# Import the Dwave packages dimod and neal\n",
    "import dimod\n",
    "import neal\n",
    "# Import Matplotlib to generate plots\n",
    "import matplotlib.pyplot as plt\n",
    "# Import numpy and scipy for certain numerical calculations below\n",
    "import numpy as np\n",
    "import math\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "from itertools import chain\n",
    "import time\n",
    "import networkx as nx\n",
    "import os\n",
    "import pickle\n",
    "from scipy import stats\n",
    "from matplotlib import ticker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These could also be simple lists and numpy matrices\n",
    "h = {0: 145.0, 1: 122.0, 2: 122.0, 3: 266.0, 4: 266.0, 5: 266.0, 6: 242.5, 7: 266.0, 8: 386.5, 9: 387.0, 10: 386.5}\n",
    "J = {(0, 3): 24.0, (0, 4): 24.0, (0, 5): 24.0, (0, 7): 24.0, (0, 8): 24.0, (0, 9): 24.0, (0, 10): 24.0, (1, 3): 24.0, (1, 5): 24.0, (1, 6): 24.0, (1, 8): 24.0, (1, 9): 24.0, (1, 10): 24.0, (2, 4): 24.0, (2, 6): 24.0, (2, 7): 24.0, (2, 8): 24.0, (2, 9): 24.0, (2, 10): 24.0, (3, 4): 24.0, (3, 5): 48.0, (3, 6): 24.0, (3, 7): 24.0, (3, 8): 48.0, (3, 9): 48.0, (3, 10): 48.0, (4, 5): 24.0, (4, 6): 24.0, (4, 7): 48.0, (4, 8): 48.0, (4, 9): 48.0, (4, 10): 48.0, (5, 6): 24.0, (5, 7): 24.0, (5, 8): 48.0, (5, 9): 48.0, (5, 10): 48.0, (6, 7): 24.0, (6, 8): 48.0, (6, 9): 48.0, (6, 10): 48.0, (7, 8): 48.0, (7, 9): 48.0, (7, 10): 48.0, (8, 9): 72.0, (8, 10): 72.0, (9, 10): 72.0}\n",
    "cI = 1319.5\n",
    "\n",
    "\n",
    "model_ising = dimod.BinaryQuadraticModel.from_ising(h, J, offset=cI) # define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx_graph = model_ising.to_networkx_graph()\n",
    "edges, bias = zip(*nx.get_edge_attributes(nx_graph, 'bias').items())\n",
    "bias = np.array(bias)\n",
    "nx.draw(nx_graph, node_size=15, pos=nx.spring_layout(nx_graph),\n",
    "        edgelist=edges, edge_color=bias, edge_cmap=plt.cm.Blues)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the problem is relatively small (11 variables, $2^{11}=2048$ combinations), we can afford to enumerate all the solutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exactSampler = dimod.reference.samplers.ExactSolver()\n",
    "start = time.time()\n",
    "exactSamples = exactSampler.sample(model_ising)\n",
    "timeEnum = time.time() - start\n",
    "print(\"runtime: \" + str(timeEnum) + \" seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some useful functions to get plots\n",
    "def plot_energy_values(results, title=None):\n",
    "\n",
    "    _, ax = plt.subplots()\n",
    "\n",
    "    energies = [datum.energy for datum in results.data(\n",
    "        ['energy'], sorted_by='energy')]\n",
    "\n",
    "    if results.vartype == 'Vartype.BINARY':\n",
    "        samples = [''.join(c for c in str(datum.sample.values()).strip(\n",
    "            ', ') if c.isdigit()) for datum in results.data(['sample'], sorted_by=None)]\n",
    "        ax.set(xlabel='bitstring for solution')\n",
    "    else:\n",
    "        samples = np.arange(len(energies))\n",
    "        ax.set(xlabel='solution')\n",
    "\n",
    "    ax.bar(samples, energies)\n",
    "    ax.tick_params(axis='x', rotation=90)\n",
    "    ax.set_ylabel('Energy')\n",
    "    if title:\n",
    "        ax.set_title(str(title))\n",
    "    print(\"minimum energy:\", min(energies))\n",
    "    return ax\n",
    "\n",
    "\n",
    "def plot_samples(results, title=None, skip=1):\n",
    "    _, ax = plt.subplots()\n",
    "    if results.vartype == 'Vartype.BINARY':\n",
    "        samples = [''.join(c for c in str(datum.sample.values()).strip(\n",
    "            ', ') if c.isdigit()) for datum in results.data(['sample'], sorted_by=None)]\n",
    "        ax.set_xlabel('bitstring for solution')\n",
    "    else:\n",
    "        samples = np.arange(len(energies))\n",
    "        ax.set_xlabel('solution')\n",
    "\n",
    "    counts = Counter(samples)\n",
    "    total = len(samples)\n",
    "    for key in counts:\n",
    "        counts[key] /= total\n",
    "    df = pd.DataFrame.from_dict(counts, orient='index').sort_index()\n",
    "    df.plot(kind='bar', legend=None, ax=ax)\n",
    "\n",
    "    ax.tick_params(axis='x', rotation=80)\n",
    "    ax.set_xticklabels([t.get_text()[:7] if not i%skip else \"\" for i,t in enumerate(ax.get_xticklabels())])\n",
    "    ax.set_ylabel('Probabilities')\n",
    "    if title:\n",
    "        ax.set_title(str(title))\n",
    "    print(\"minimum energy:\", min(energies))\n",
    "    return ax\n",
    "\n",
    "\n",
    "def plot_energy_cfd(results, title=None, skip=1):\n",
    "    _, ax = plt.subplots()\n",
    "    # skip parameter given to avoid putting all xlabels\n",
    "    energies = results.data_vectors['energy']\n",
    "    occurrences = results.data_vectors['num_occurrences']\n",
    "    counts = Counter(energies)\n",
    "    total = sum(occurrences)\n",
    "    counts = {}\n",
    "    for index, energy in enumerate(energies):\n",
    "        if energy in counts.keys():\n",
    "            counts[energy] += occurrences[index]\n",
    "        else:\n",
    "            counts[energy] = occurrences[index]\n",
    "    for key in counts:\n",
    "        counts[key] /= total\n",
    "    df = pd.DataFrame.from_dict(counts, orient='index').sort_index()\n",
    "    df.plot(kind='bar', legend=None, ax = ax)\n",
    "    ax.set_xticklabels([t.get_text()[:7] if not i%skip else \"\" for i,t in enumerate(ax.get_xticklabels())])\n",
    "\n",
    "    ax.set_xlabel('Energy')\n",
    "    ax.set_ylabel('Probabilities')\n",
    "    if title:\n",
    "        ax.set_title(str(title))\n",
    "    print(\"minimum energy:\", min(energies))\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_energy_values(exactSamples, title='Enumerate all solutions')\n",
    "plot_energy_cfd(exactSamples, title='Enumerate all solutions', skip=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that the optimal solution of this problem is $x_{10} = 1, 0$ otherwise, leading to an objective of $5$. Notice that this problem has a degenerate optimal solution given that $x_8 = 1, 0$ otherwise also leads to the same solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now solve this problem using Simulated Annealing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simAnnSampler = neal.SimulatedAnnealingSampler()\n",
    "simAnnSamples = simAnnSampler.sample(model_ising, num_reads=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_energy_values(simAnnSamples, title='Simulated annealing in default parameters')\n",
    "plot_energy_cfd(simAnnSamples, title='Simulated annealing in default parameters')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to use the default limits of temperature given by the simulating annealing code. These are defined using the minimum and maximum nonzero coefficients in the Ising model. Then the range for beta is defined as \n",
    "\n",
    "$$\n",
    "\\beta \\in \\left[ \\frac{\\log(2)}{\\max \\{ \\Delta E \\} },\\frac{\\log(100)}{\\min \\{ \\Delta E \\} } \\right]\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "\\Delta E = \\min_{i} \\{h_i \\} + \\sum_j J_{ij}+J_{ji}\n",
    "$$\n",
    "\n",
    "Hot temperature: We want to scale hot_beta so that for the most unlikely qubit flip, we get at least 50% chance of flipping. (This means all other qubits will have > 50% chance of flipping initially). Most unlikely flip is when we go from a very low energy state to a high energy state, thus we calculate hot_beta based on max_delta_energy.\n",
    "\n",
    "$$\n",
    "0.50 = \\exp(-\\overline{\\beta} * \\max \\{ \\Delta E \\})\n",
    "$$\n",
    "\n",
    "Cold temperature: Towards the end of the annealing schedule, we want to minimize the chance of flipping. Don't want to be stuck between small energy tweaks. Hence, set cold_beta so that at minimum energy change, the chance of flipping is set to 1%.\n",
    "\n",
    "$$\n",
    "0.01 = \\exp(-\\underline{\\beta} * \\min \\{ \\Delta E \\})\n",
    "$$\n",
    "\n",
    "By default, the schedule also follows a geometric series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def geomspace(a, b, length=100):\n",
    "    return np.logspace(np.log10(a), np.log10(b), num=length, endpoint=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hex_to_RGB(hex_str):\n",
    "    return [int(hex_str[i:i+2], 16) for i in range(1,6,2)]\n",
    "\n",
    "def get_color_gradient(c1, c2, n):\n",
    "    assert n > 1\n",
    "    c1_rgb = np.array(hex_to_RGB(c1))/255\n",
    "    c2_rgb = np.array(hex_to_RGB(c2))/255\n",
    "    mix_pcts = [x/(n-1) for x in range(n)]\n",
    "    rgb_colors = [((1-mix)*c1_rgb + (mix*c2_rgb)) for mix in mix_pcts]\n",
    "    return [\"#\" + \"\".join([format(int(round(val*255)), \"02x\") for val in item]) for item in rgb_colors]\n",
    "\n",
    "def plot_schedule(beta1, beta2, length=1000):\n",
    "    color1 = \"#00008b\"\n",
    "    color2 = \"#D22B2B\"\n",
    "    sweeps = np.linspace(np.log10(2), np.log10(100), num=length, endpoint=True)\n",
    "    beta = np.geomspace(beta1, beta2, num=length)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.scatter(x=sweeps, \n",
    "                y=beta, \n",
    "                color = get_color_gradient(color1, color2, len(beta)), \n",
    "                s=10)\n",
    "    plt.title(\"Default Geometric temperature schedule\")\n",
    "    plt.xlabel(\"Sweeps\")\n",
    "    plt.ylabel(r\"$ \\beta $ = Inverse temperature\")\n",
    "    plt.xticks([sweeps[0], sweeps[249], sweeps[499], sweeps[749], sweeps[999]], [0,250,500,750,1000])\n",
    "    plt.yticks([beta1, beta2], [r\"$ \\beta_{0} $\", r\"$ \\beta_{1} $\"])\n",
    "    plt.grid(False)\n",
    "    plt.show()\n",
    "\n",
    "beta1 = 0.1\n",
    "beta2 = 10\n",
    "plot_schedule(beta1, beta2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's compute an expected time metric with respect to the number of sweeps in simulated annealing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = 0.99\n",
    "# sweeps = list(chain(np.arange(1,10,1),np.arange(10,30,2), np.arange(30,50,5), np.arange(50,100,10) ,np.arange(100,1001,100)))\n",
    "sweeps = list(chain(np.arange(1, 250, 1), np.arange(250, 1001, 10)))\n",
    "schedules = ['geometric','linear']\n",
    "opt_energy = 5\n",
    "results = {}\n",
    "results['p'] = {}\n",
    "results['tts'] = {}\n",
    "results['t']= {}\n",
    "for schedule in schedules:\n",
    "    probs = []\n",
    "    time_to_sol = []\n",
    "    times = []\n",
    "    for sweep in sweeps:\n",
    "        start = time.time()\n",
    "        samples = simAnnSampler.sample(model_ising, num_reads=1000, num_sweeps=sweep, beta_schedule_type=schedule)\n",
    "        time_s = time.time() - start\n",
    "        energies=samples.data_vectors['energy']\n",
    "        occurrences = samples.data_vectors['num_occurrences']\n",
    "        total_counts = sum(occurrences)\n",
    "        counts = {}\n",
    "        for index, energy in enumerate(energies):\n",
    "            if energy in counts.keys():\n",
    "                counts[energy] += occurrences[index]\n",
    "            else:\n",
    "                counts[energy] = occurrences[index]\n",
    "        pr = sum(counts[key]\n",
    "                 for key in counts.keys() if key <= opt_energy)/total_counts\n",
    "        probs.append(pr)\n",
    "        if pr == 0:\n",
    "            time_to_sol.append(np.inf)\n",
    "        else:\n",
    "            time_to_sol.append(time_s*math.log10(1-s)/math.log10(1-pr))\n",
    "        times.append(time_s)\n",
    "    results['p'][schedule] = probs\n",
    "    results['tts'][schedule] = time_to_sol\n",
    "    results['t'][schedule] = times\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(2)\n",
    "fig.suptitle('Simulated annealing expected runtime of \\n' +\n",
    "             ' easy Ising N=10 with varying schedule and sweeps')\n",
    "\n",
    "for schedule in schedules:\n",
    "    ax1.plot(sweeps, results['t'][schedule], '-', label=schedule)\n",
    "ax1.hlines(results['t']['geometric'][-1], sweeps[0], sweeps[-1],\n",
    "           linestyle='--', label='default', colors='b')\n",
    "ax1.hlines(timeEnum, sweeps[0], sweeps[-1], linestyle='--', label='enumerate')\n",
    "\n",
    "ax1.set(ylabel='Time [s]')\n",
    "\n",
    "for schedule in schedules:\n",
    "    ax2.plot(sweeps, results['p'][schedule], '-', label=schedule)\n",
    "ax2.hlines(results['p']['geometric'][-1], sweeps[0], sweeps[-1],\n",
    "           linestyle='--', label='default', colors='b')\n",
    "ax2.hlines(1, sweeps[0], sweeps[-1], linestyle='--', label='enumerate')\n",
    "\n",
    "ax2.set(ylabel='Success Probability [%]')\n",
    "ax2.set(xlabel='Sweeps')\n",
    "\n",
    "plt.legend(ncol=2, loc='upper center', bbox_to_anchor=(0.5, -0.25))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These plots represent ofter contradictory metrics, on one hand you would like to obtain a large probability of finding a right solution (the deffinition of right comes from what you define as success). On the other hand, the time it takes to solve these cases should be as small as possible.\n",
    "This is why we are interested in a metric that combines both, and that is why we settle on the Time To Solution (TTS) which is defined as\n",
    "$$\n",
    "TTS = \\frac{\\log{1-s}}{\\log{1-p}}\n",
    "$$\n",
    "where s is a success factor, usually takes as $s = 99\\%$, and $p$ is the success probability, usually accounted as the observed success probability.\n",
    "\n",
    "One usually reads this as the time to solution within 99\\% probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1 = plt.figure()\n",
    "ax1 = fig1.add_subplot(111)\n",
    "for schedule in schedules:\n",
    "    ax1.semilogy(sweeps, results['tts'][schedule], '-', label=schedule)\n",
    "\n",
    "\n",
    "# Value for the default solution\n",
    "ttsDefault = results['tts']['geometric'][-1]\n",
    "ax1.hlines(ttsDefault, sweeps[0], sweeps[-1], linestyle='--', label='default', colors='b')\n",
    "\n",
    "\n",
    "ax1.set_ylabel('Time To Optimal Solution with ' + str(s*100) +' % chance [s]')\n",
    "ax1.set_xlabel('Sweeps')\n",
    "ax1.set_title('Simulated annealing expected runtime of \\n' + ' example with varying schedule and sweeps')\n",
    "\n",
    "ax2 = plt.axes([.45, .3, .4, .4])\n",
    "for schedule in schedules:\n",
    "    ax2.semilogy(sweeps[0:5],results['tts'][schedule][0:5],'-s')\n",
    "ax2.set_ylabel('TTS [s]')\n",
    "ax2.set_xlabel('Sweeps')\n",
    "\n",
    "\n",
    "ax1.legend(ncol = 2, loc='upper center', bbox_to_anchor=(0.5, -0.15))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can notice, the default parameters given by D-Wave (number of sweeps = 1000 and a geometric update of $\\beta$) are not optimal for our tiny example in terms of expected runtime.\n",
    "This is certainly a function of the problem, for such a small instance having two sweeps are more than enough and more sweeps are an overkill.\n",
    "This parameters choice might not generalize to any other problem, as seen below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2\n",
    "Let's define a larger model, with 100 variables and random weights, to see how this performance changes.\n",
    "\n",
    "Assume that we are interested at the instance created with random weights $h_{i}, J_{i, j} \\sim U[-1, +1]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 100 # Number of variables\n",
    "np.random.seed(42) # Fixing the random seed to get the same result\n",
    "J = np.random.rand(N,N)\n",
    "J = np.triu(J, 1) # We only consider upper triangular matrix ignoring the diagonal\n",
    "h = np.random.rand(N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_random = dimod.BinaryQuadraticModel.from_ising(h, J, offset=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx_graph = model_random.to_networkx_graph()\n",
    "edges, bias = zip(*nx.get_edge_attributes(nx_graph, 'bias').items())\n",
    "bias = np.array(bias)\n",
    "nx.draw(nx_graph, node_size=15, pos=nx.spring_layout(nx_graph), alpha=0.25, edgelist=edges, edge_color=bias, edge_cmap=plt.cm.Blues)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a problem of this size we cannot do a complete enumeration ($2^{100} \\approx 1.2e30$) but we can randomly sample the distribution of energies to have a baseline for our later comparisons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "randomSampler = dimod.RandomSampler()\n",
    "randomSample = randomSampler.sample(model_random, num_reads=1000)\n",
    "energies = [datum.energy for datum in randomSample.data(\n",
    "        ['energy'], sorted_by='energy')]\n",
    "random_energy = np.mean(energies)\n",
    "print('Average random energy:' + str(random_energy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_energy_values(randomSample,\n",
    "               title='Random sampling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "simAnnSamplesDefault = simAnnSampler.sample(model_random, num_reads=1000)\n",
    "timeDefault = time.time() - start\n",
    "energies = [datum.energy for datum in simAnnSamplesDefault.data(\n",
    "        ['energy'], sorted_by='energy')]\n",
    "min_energy = energies[0]\n",
    "print(\"minimum energy: \" + str(min_energy))\n",
    "print(\"runtime: \" + str(timeDefault) + \" seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem statement\n",
    "\n",
    "We pose the Ising problem as the following optimization problem:\n",
    "\n",
    "$$\n",
    "\\min_{s \\in \\{ \\pm 1 \\}^n} H(s) = \\min_{s \\in \\{ \\pm 1 \\}^n} \\sum_{(i, j) \\in E(G)} J_{i,j}s_is_j + \\sum_{i \\in V(G)} h_is_i + \\beta\n",
    "$$\n",
    "\n",
    "where we optimize over spins $s \\in \\{ \\pm 1 \\}^n$, on a constrained graph $G(V,E)$, where the quadratic coefficients are $J_{i,j}$ and the linear coefficients are $h_i$.\n",
    "We also include an arbitrary offset of the Ising model $\\beta$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax_enum = plot_energy_values(simAnnSamplesDefault,\n",
    "               title='Simulated annealing with default parameters')\n",
    "ax_enum.set(ylim=[min_energy*(0.99)**np.sign(min_energy),min_energy*(1.1)**np.sign(min_energy)])\n",
    "plot_energy_cfd(simAnnSamplesDefault,\n",
    "              title='Simulated annealing with default parameters', skip=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the minimum energy coming from the random sampling and the one from the simulated annealing are very different. \n",
    "Moreover, the distributions that both lead to are extremely different too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(simAnnSamplesDefault.info)\n",
    "beta_schedule = np.geomspace(*simAnnSamplesDefault.info['beta_range'], num=1000)\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(beta_schedule,'.')\n",
    "ax.set_xlabel('Sweeps')\n",
    "ax.set_ylabel('beta=Inverse temperature')\n",
    "ax.set_title('Default Geometric temperature schedule')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can solve this problem using IP such that we have guarantees that it is solved to optimality (this might be a great quiz for future lectures), but in this case let us define the \"success\" as getting an objective certain percentage of the best found solution in all cases (which we see it might not be even found with the default parameters).\n",
    "To get a scaled version of this success equivalent for all instances, we will define this success with respect to the metric:\n",
    "$$\n",
    "\\frac{found - random}{minimum - random}\n",
    "$$\n",
    "Where $found$ corresponds to the best found solution within our sampling, $random$ is the mean of the random sampling shown above, and $minimum$ corresponds to the best found solution to our problem during the exploration. Consider that this minimum might not be the global minimum.\n",
    "This metric is very informative given that the best performance you can have is 1, being at the minimum, and negative values would correspond to a method that at best behaves worse that the random sampling.\n",
    "Success now is counted as being within certain treshold of this value of 1.\n",
    "This new way of measuring each method is very similar to the approximation ratio of approximation algorithms, therefore we will use that terminology from now on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before figuring out if we have the right optimal parameters, we want to save some effort by loading previously computed results.\n",
    "If you do not want to load the results that we are providing, feel free to change the `overwrite_pickles` variable, at the expense that it will take some time (around 10 minutes per instance) to run.\n",
    "Otherwise, a zip file with precomputed results will be downloaded from github."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_path = os.getcwd()\n",
    "pickle_path = os.path.join(current_path, 'results/')\n",
    "if not(os.path.exists(pickle_path)):\n",
    "    print('Results directory ' + pickle_path +\n",
    "          ' does not exist. We will create it.')\n",
    "    os.makedirs(pickle_path)\n",
    "    !wget -O /content/results/results.zip -N -q \"https://github.com/SECQUOIA/QUBONotebooks/raw/main/notebooks_py/results.zip\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "zip_name = os.path.join(pickle_path, 'results.zip')\n",
    "overwrite_pickles = False\n",
    "if os.path.exists(zip_name):\n",
    "    with zipfile.ZipFile(zip_name, 'r') as zip_ref:\n",
    "        zip_ref.extractall(pickle_path)\n",
    "    print('Results zip file has been extrated to ' + pickle_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now either we have the pickled file or not, let us compute the statistics we are looking for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = 0.99 # This is the success probability for the TTS calculation\n",
    "treshold = 5.0 # This is a percentual treshold of what the minimum energy should be\n",
    "sweeps = list(chain(np.arange(1, 250, 1), np.arange(250, 1001, 10)))\n",
    "# schedules = ['geometric', 'linear']\n",
    "schedules = ['geometric']\n",
    "total_reads = 1000\n",
    "default_sweeps = 1000\n",
    "n_boot=1000\n",
    "ci=68 # Confidence interval for bootstrapping\n",
    "boots = [1, 10, default_sweeps]\n",
    "min_energy = -239.5\n",
    "instance = 42    \n",
    "results_name = \"results_\" + str(instance) + \".pkl\"\n",
    "results_name = os.path.join(pickle_path, results_name)\n",
    "results = {}\n",
    "results['p'] = {}\n",
    "results['min_energy'] = {}\n",
    "results['random_energy'] = {}\n",
    "results['tts'] = {}\n",
    "results['ttsci'] = {}\n",
    "results['t']= {}\n",
    "results['best'] = {}\n",
    "results['bestci'] = {}\n",
    "# If you wanto to use the raw data and process it here\n",
    "if not(os.path.exists(results_name)):\n",
    "    # If you want to generate the data or load it here\n",
    "\n",
    "    for boot in boots:\n",
    "        results['p'][boot] = {}\n",
    "        results['tts'][boot] = {}\n",
    "        results['ttsci'][boot] = {}\n",
    "        results['best'][boot] = {}\n",
    "        results['bestci'][boot] = {}\n",
    "\n",
    "    for schedule in schedules:\n",
    "        probs = {k: [] for k in boots}\n",
    "        time_to_sol = {k: [] for k in boots}\n",
    "        prob_np = {k: [] for k in boots}\n",
    "        ttscs = {k: [] for k in boots}\n",
    "        times = []\n",
    "        b = {k: [] for k in boots}\n",
    "        bnp = {k: [] for k in boots}\n",
    "        bcs = {k: [] for k in boots}\n",
    "        for sweep in sweeps:\n",
    "            # Gather instance names\n",
    "            pickle_name = str(instance) + \"_\" + schedule + \"_\" + str(sweep) + \".p\"\n",
    "            pickle_name = os.path.join(pickle_path, pickle_name)\n",
    "            # If the instance data exists, load the data\n",
    "            if os.path.exists(pickle_name) and not overwrite_pickles:\n",
    "                # print(pickle_name)\n",
    "                samples = pickle.load(open(pickle_name, \"rb\"))\n",
    "                time_s = samples.info['timing']\n",
    "            # If it does not exist, generate the data\n",
    "            else:\n",
    "                start = time.time()\n",
    "                samples = simAnnSampler.sample(model_random, num_reads=total_reads, num_sweeps=sweep, beta_schedule_type=schedule)\n",
    "                time_s = time.time() - start\n",
    "                samples.info['timing'] = time_s\n",
    "                pickle.dump(samples, open(pickle_name, \"wb\"))\n",
    "            # Compute statistics\n",
    "            energies=samples.data_vectors['energy']\n",
    "            occurrences = samples.data_vectors['num_occurrences']\n",
    "            total_counts = sum(occurrences)\n",
    "            times.append(time_s)\n",
    "            if min(energies) < min_energy:\n",
    "                min_energy = min(energies)\n",
    "                print(\"A better solution of \" + str(min_energy) + \" was found for sweep \" + str(sweep))\n",
    "            # success = min_energy*(1.0 + treshold/100.0)**np.sign(min_energy)\n",
    "            success = random_energy - (random_energy - min_energy)*(1.0 - treshold/100.0)\n",
    "            \n",
    "            # Best of boot samples es computed via n_boot bootstrappings\n",
    "            boot_dist = {}\n",
    "            pr_dist = {}\n",
    "            cilo = {}\n",
    "            ciup = {}\n",
    "            pr = {}\n",
    "            pr_cilo = {}\n",
    "            pr_ciup = {}\n",
    "            for boot in boots:\n",
    "                boot_dist[boot] = []\n",
    "                pr_dist[boot] = []\n",
    "                for i in range(int(n_boot)):\n",
    "                    resampler = np.random.randint(0, total_reads, boot)\n",
    "                    sample_boot = energies.take(resampler, axis=0)\n",
    "                    # Compute the best along that axis\n",
    "                    boot_dist[boot].append(min(sample_boot))\n",
    "    \n",
    "                    occurences = occurrences.take(resampler, axis=0)\n",
    "                    counts = {}\n",
    "                    for index, energy in enumerate(sample_boot):\n",
    "                        if energy in counts.keys():\n",
    "                            counts[energy] += occurences[index]\n",
    "                        else:\n",
    "                            counts[energy] = occurences[index]\n",
    "                    pr_dist[boot].append(sum(counts[key] for key in counts.keys() if key < success)/boot)   \n",
    "                \n",
    "                b[boot].append(np.mean(boot_dist[boot]))\n",
    "                # Confidence intervals from bootstrapping the best out of boot\n",
    "                bnp[boot] = np.array(boot_dist[boot])\n",
    "                cilo[boot] = np.apply_along_axis(stats.scoreatpercentile, 0, bnp[boot], 50.-ci/2.)\n",
    "                ciup[boot] = np.apply_along_axis(stats.scoreatpercentile, 0, bnp[boot], 50.+ci/2.)  \n",
    "                bcs[boot].append((cilo[boot],ciup[boot]))\n",
    "                # Confidence intervals from bootstrapping the TTS of boot\n",
    "                prob_np[boot] = np.array(pr_dist[boot])\n",
    "                pr[boot] = np.mean(prob_np[boot])\n",
    "                probs[boot].append(pr[boot])\n",
    "                if prob_np[boot].all() == 0:\n",
    "                    time_to_sol[boot].append(np.inf)\n",
    "                    ttscs[boot].append((np.inf, np.inf))\n",
    "                else:\n",
    "                    pr_cilo[boot] = np.apply_along_axis(stats.scoreatpercentile, 0, prob_np[boot], 50.-ci/2.)\n",
    "                    pr_ciup[boot] = np.apply_along_axis(stats.scoreatpercentile, 0, prob_np[boot], 50.+ci/2.)\n",
    "                    time_to_sol[boot].append(time_s*math.log10(1-s)/math.log10(1-pr[boot]+1e-9)) \n",
    "                    ttscs[boot].append((time_s*math.log10(1-s)/math.log10(1-pr_cilo[boot]),time_s*math.log10(1-s)/math.log10(1-pr_ciup[boot]+1e-9)))\n",
    "            \n",
    "        results['t'][schedule] = times\n",
    "        results['min_energy'][schedule] = min_energy\n",
    "        results['random_energy'][schedule] = random_energy\n",
    "        for boot in boots:\n",
    "            results['p'][boot][schedule] = probs[boot]\n",
    "            results['tts'][boot][schedule] = time_to_sol[boot]\n",
    "            results['ttsci'][boot][schedule] = ttscs[boot]\n",
    "            results['best'][boot][schedule] = [(random_energy - energy) / (random_energy - min_energy) for energy in b[boot]]\n",
    "            results['bestci'][boot][schedule] = [tuple((random_energy - element) / (random_energy - min_energy) for element in energy) for energy in bcs[boot]]\n",
    "    \n",
    "    # Save results file in case that we are interested in reusing them\n",
    "    pickle.dump(results, open(results_name, \"wb\"))\n",
    "else: # Just reload processed datafile\n",
    "    results = pickle.load(open(results_name, \"rb\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After gathering all the results, we would like to see the progress of the approximation ration with respect to the increasing number of sweeps. \n",
    "To account for the stochasticity of this method, we are bootstrapping all of our results with different values of the bootstrapping sample, and each confidence interval corresponds to a standard deviation away from the mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "for boot in boots:\n",
    "    for schedule in schedules:\n",
    "        ax.plot(sweeps,results['best'][boot][schedule], label=str(schedule) + ', ' + str(boot) + ' reads')\n",
    "        bestnp = np.stack(results['bestci'][boot][schedule], axis=0).T\n",
    "        ax.fill_between(sweeps,bestnp[0],bestnp[1],alpha=0.25)\n",
    "ax.set(xlabel='Sweeps')\n",
    "ax.set(ylabel='Approximation ratio = \\n ' + '(best found - random sample) / (min energy - random sample)')\n",
    "ax.set_title('Simulated annealing approximation ratio of Ising 42 N=100\\n' +\n",
    "          ' with varying schedule, ' + str(n_boot) + ' bootstrap re-samples, and sweeps')\n",
    "plt.legend(ncol=3, loc='upper center', bbox_to_anchor=(0.5, -0.15))\n",
    "ax.set(xscale='log')\n",
    "ax.set(ylim=[0.8,1.01])\n",
    "# ax.set(xlim=[1,200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, besides looking at the sweeps, which are our parameter, we want to see how the performance changes with respect to the number of shots, which in this case are proportional to the computational time/effort that it takes to solve the problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "for boot in boots:\n",
    "    reads = [s * boot for s in sweeps]\n",
    "    for schedule in schedules:\n",
    "        ax.plot(reads,results['best'][boot][schedule], label=str(schedule) + ' with ' + str(boot) + ' reads')\n",
    "        bestnp = np.stack(results['bestci'][boot][schedule], axis=0).T\n",
    "        ax.fill_between(reads,bestnp[0],bestnp[1],alpha=0.25)\n",
    "ax.set(xlabel='Total number of reads')\n",
    "ax.set(ylabel='Approximation ratio = \\n ' + '(best found - random sample) / (min energy - random sample)')\n",
    "ax.set_title('Simulated annealing approximation ratio of Ising 42 N=100\\n' +\n",
    "          ' with varying schedule, ' + str(n_boot) + ' bootstrap re-samples, and sweeps')\n",
    "plt.legend(ncol=3, loc='upper center', bbox_to_anchor=(0.5, -0.15))\n",
    "ax.set(xscale='log')\n",
    "ax.set(ylim=[0.8,1.01])\n",
    "# ax.set(xlim=[1,200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1,ax2) = plt.subplots(2)\n",
    "fig.suptitle('Simulated annealing expected runtime of \\n' +\n",
    "             ' random Ising N=100 with varying schedule and sweeps')\n",
    "\n",
    "for schedule in schedules:\n",
    "    ax1.plot(sweeps, results['t'][schedule], '-', label=schedule)\n",
    "ax1.hlines(results['t']['geometric'][-1], sweeps[0], sweeps[-1],\n",
    "           linestyle='--', label='default', colors='b')\n",
    "\n",
    "ax1.set(ylabel='Time [s]')\n",
    "# ax1.set(xlim=[1,200])\n",
    "\n",
    "\n",
    "for schedule in schedules:\n",
    "    ax2.semilogy(sweeps, results['p'][default_sweeps][schedule], '-', label=schedule)\n",
    "ax2.hlines(results['p'][default_sweeps]['geometric'][-1], sweeps[0], sweeps[-1],\n",
    "           linestyle='--', label='default', colors='b')\n",
    "# ax2.set(xlim=[1,200])\n",
    "\n",
    "ax2.set(ylabel='Success Probability \\n (within '+ str(treshold) +' % of best found)')\n",
    "ax2.set(xlabel='Sweeps')\n",
    "plt.legend(ncol=3, loc='upper center', bbox_to_anchor=(0.5, -0.3))\n",
    "\n",
    "# Add plot going all the way to 1000 sweeps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1, ax1 = plt.subplots()\n",
    "\n",
    "for boot in reversed(boots):\n",
    "    for schedule in schedules:\n",
    "        ax1.plot(sweeps,results['tts'][boot][schedule], label=schedule + \"_boot\" + str(boot))\n",
    "        ttsnp = np.stack(results['ttsci'][boot][schedule], axis=0).T\n",
    "        ax1.fill_between(sweeps,ttsnp[0],ttsnp[1],alpha=0.25)\n",
    "\n",
    "\n",
    "ax1.hlines(results['tts'][total_reads]['geometric'][-1], sweeps[0], sweeps[-1],\n",
    "           linestyle='--', label='default', colors='b')\n",
    "\n",
    "ax1.set(yscale='log')\n",
    "ax1.set(ylim=[3,1e3])\n",
    "# ax1.set(xlim=[1,200])\n",
    "\n",
    "ax1.set(ylabel='Time To Solution within '+ str(treshold) +' % of best found [s]')\n",
    "ax1.set(xlabel='Sweeps')\n",
    "ax.set_title('Simulated annealing expected runtime of random Ising N=100\\n' +\n",
    "          ' with varying schedule, ' + str(n_boot) + ' bootstrap re-samples, and sweeps')\n",
    "\n",
    "plt.legend(loc='upper center', bbox_to_anchor=(0.5, -0.2),\n",
    "          ncol=2, fancybox=False, shadow=False)\n",
    "\n",
    "ax2 = plt.axes([.45, .55, .4, .3])\n",
    "for schedule in schedules:\n",
    "    min_tts = min(results['tts'][default_sweeps][schedule])\n",
    "    min_index = results['tts'][default_sweeps][schedule].index(min_tts)\n",
    "    min_sweep = sweeps[results['tts'][default_sweeps][schedule].index(min_tts)]\n",
    "    print(\"minimum TTS for \" + schedule + \" schedule = \" + str(min_tts) + \"s at sweep = \" + str(min_sweep))\n",
    "    for boot in reversed(boots):\n",
    "        ax2.semilogy(sweeps[min_index-10:min_index+10], results['tts'][boot][schedule][min_index-10:min_index+10], '-s')\n",
    "ax2.hlines(results['tts'][default_sweeps]['geometric'][-1], sweeps[min_index-10], sweeps[min_index+10],\n",
    "           linestyle='--', label='default', colors='b')\n",
    "\n",
    "ax2.set(ylabel='TTS [s]')\n",
    "ax2.set(xlabel='Sweeps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_beta_schedule = np.geomspace(*simAnnSamplesDefault.info['beta_range'], num=min_sweep)\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(beta_schedule,'.')\n",
    "ax.plot(min_beta_schedule,'.')\n",
    "ax.set_xlabel('Sweeps')\n",
    "ax.set_ylabel('beta=Inverse temperature')\n",
    "ax.set_title('Geometric temperature schedule')\n",
    "plt.legend(['Default','Best'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "# Best of boot samples es computed via n_boot bootstrapping\n",
    "# n_boot=500\n",
    "boots = range(1,1000,1)\n",
    "interest_sweeps = [min_sweep, default_sweeps, 10, 500]\n",
    "approx_ratio = {}\n",
    "approx_ratioci = {}\n",
    "\n",
    "for schedul in schedules:\n",
    "    approx_ratio[schedule] = {}\n",
    "    approx_ratioci[schedule] = {}\n",
    "\n",
    "# Gather instance names\n",
    "instance = 42\n",
    "for sweep in interest_sweeps:\n",
    "    for schedule in schedules:\n",
    "        if sweep in approx_ratio[schedule] and sweep in approx_ratioci[schedule]:\n",
    "            pass\n",
    "        else:\n",
    "            min_energy = results['min_energy'][schedule]\n",
    "            random_energy = results['random_energy'][schedule]\n",
    "\n",
    "            pickle_name = str(instance) + \"_\" + schedule + \"_\" + str(sweep) + \".p\"\n",
    "            pickle_name = os.path.join(pickle_path, pickle_name)\n",
    "            # If the instance data exists, load the data\n",
    "            if os.path.exists(pickle_name) and not overwrite_pickles:\n",
    "                # print(pickle_name)\n",
    "                samples = pickle.load(open(pickle_name, \"rb\"))\n",
    "                time_s = samples.info['timing']\n",
    "            # If it does not exist, generate the data\n",
    "            else:\n",
    "                start = time.time()\n",
    "                samples = simAnnSampler.sample(model_random, num_reads=total_reads, num_sweeps=sweep, beta_schedule_type=schedule)\n",
    "                time_s = time.time() - start\n",
    "                samples.info['timing'] = time_s\n",
    "                pickle.dump(samples, open(pickle_name, \"wb\"))\n",
    "            # Compute statistics\n",
    "            energies=samples.data_vectors['energy']\n",
    "            if min(energies) < min_energy:\n",
    "                min_energy = min(energies)\n",
    "                print(\"A better solution of \" + str(min_energy) + \" was found for sweep \" + str(sweep))\n",
    "\n",
    "            b = []\n",
    "            bcs = []\n",
    "            probs = []\n",
    "            time_to_sol = []\n",
    "            for boot in boots:\n",
    "                boot_dist = []\n",
    "                pr_dist = []\n",
    "                for i in range(int(n_boot - boot + 1)):\n",
    "                    resampler = np.random.randint(0, total_reads, boot)\n",
    "                    sample_boot = energies.take(resampler, axis=0)\n",
    "                    # Compute the best along that axis\n",
    "                    boot_dist.append(min(sample_boot))   \n",
    "                \n",
    "                b.append(np.mean(boot_dist))\n",
    "                # Confidence intervals from bootstrapping the best out of boot\n",
    "                bnp = np.array(boot_dist)\n",
    "                cilo = np.apply_along_axis(stats.scoreatpercentile, 0, bnp, 50.-ci/2.)\n",
    "                ciup = np.apply_along_axis(stats.scoreatpercentile, 0, bnp, 50.+ci/2.)  \n",
    "                bcs.append((cilo,ciup))\n",
    "\n",
    "            approx_ratio[schedule][sweep] = [(random_energy - energy) / (random_energy - min_energy) for energy in b]\n",
    "            approx_ratioci[schedule][sweep] = [tuple((random_energy - element) / (random_energy - min_energy) for element in energy) for energy in bcs]\n",
    "\n",
    "        ax.plot([shot*sweep for shot in boots], approx_ratio[schedule][sweep], label=str(sweep) + ' sweeps')\n",
    "        approx_ratio_bestci_np = np.stack(approx_ratioci[schedule][sweep], axis=0).T\n",
    "        ax.fill_between([shot*sweep for shot in boots],approx_ratio_bestci_np[0],approx_ratio_bestci_np[1],alpha=0.25)\n",
    "ax.set(xscale='log')\n",
    "ax.set(ylim=[0.9,1.01])\n",
    "ax.set(xlim=[1e2,1e4])\n",
    "ax.set(xlabel='Total number of reads (equivalent to time)')\n",
    "ax.set(ylabel='Approximation ratio = \\n ' + '(best found - random sample) / (min energy - random sample)')\n",
    "ax.set_title('Simulated annealing approximation ratio of Ising 42 N=100\\n' +\n",
    "          ' with varying schedule, ' + str(n_boot) + ' bootstrap re-samples, and sweeps')\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here see how using the optimal number of sweeps is better than using other values (including the default recommended by the solver) in terms of solving this problem.\n",
    "Obviously, we only know this after running the experiments and verifying it ourselves. This is not the usual case, so we want to see how well can we do if we solve similar (but no the same instances).\n",
    "Here we will generate 20 random instances from the same distribution and size but different random seed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "s = 0.99  # This is the success probability for the TTS calculation\n",
    "treshold = 5.0  # This is a percentual treshold of what the minimum energy should be\n",
    "sweeps = list(chain(np.arange(1, 250, 1), np.arange(250, 1001, 10)))\n",
    "# schedules = ['geometric', 'linear']\n",
    "schedules = ['geometric']\n",
    "total_reads = 1000\n",
    "default_sweeps = 1000\n",
    "# boots = [1, 10, 100, default_sweeps]\n",
    "boots = [1, 10, default_sweeps]\n",
    "all_results = {}\n",
    "instances = range(20)\n",
    "\n",
    "all_results_name = \"all_results.pkl\"\n",
    "all_results_name = os.path.join(pickle_path, all_results_name)\n",
    "# If you wanto to use the raw data and process it here\n",
    "if not(os.path.exists(all_results_name)):\n",
    "\n",
    "    for instance in instances:\n",
    "        all_results[instance] = {}\n",
    "        all_results[instance]['p'] = {}\n",
    "        all_results[instance]['min_energy'] = {}\n",
    "        all_results[instance]['random_energy'] = {}\n",
    "        all_results[instance]['tts'] = {}\n",
    "        all_results[instance]['ttsci'] = {}\n",
    "        all_results[instance]['t']= {}\n",
    "        all_results[instance]['best'] = {}\n",
    "        all_results[instance]['bestci'] = {}\n",
    "\n",
    "        np.random.seed(instance)  # Fixing the random seed to get the same result\n",
    "        J = np.random.rand(N, N)\n",
    "        # We only consider upper triangular matrix ignoring the diagonal\n",
    "        J = np.triu(J, 1)\n",
    "        h = np.random.rand(N)\n",
    "        model_random = dimod.BinaryQuadraticModel.from_ising(h, J, offset=0.0)\n",
    "\n",
    "        randomSample = randomSampler.sample(model_random, num_reads=total_reads)\n",
    "        random_energies = [datum.energy for datum in randomSample.data(\n",
    "                ['energy'])]\n",
    "        random_energy = np.mean(random_energies)\n",
    "\n",
    "        default_pickle_name = str(instance) + \"_geometric_1000.p\"\n",
    "        default_pickle_name = os.path.join(pickle_path, default_pickle_name)\n",
    "        if os.path.exists(default_pickle_name) and not overwrite_pickles:\n",
    "            simAnnSamplesDefault = pickle.load(open(default_pickle_name, \"rb\"))\n",
    "            timeDefault = simAnnSamplesDefault.info['timing']\n",
    "        else:\n",
    "            start = time.time()\n",
    "            simAnnSamplesDefault = simAnnSampler.sample(model_random, num_reads=1000)\n",
    "            timeDefault = time.time() - start\n",
    "            simAnnSamplesDefault.info['timing'] = timeDefault\n",
    "            pickle.dump(simAnnSamplesDefault, open(default_pickle_name, \"wb\"))\n",
    "        energies = [datum.energy for datum in simAnnSamplesDefault.data(\n",
    "            ['energy'], sorted_by='energy')]\n",
    "        min_energy = energies[0]\n",
    "        for schedule in schedules:\n",
    "\n",
    "            all_results[instance]['t'][schedule] = {}\n",
    "            all_results[instance]['min_energy'][schedule] = {}\n",
    "            all_results[instance]['random_energy'][schedule] = {}\n",
    "            all_results[instance]['p'][schedule] = {}\n",
    "            all_results[instance]['tts'][schedule] = {}\n",
    "            all_results[instance]['ttsci'][schedule] = {}\n",
    "            all_results[instance]['best'][schedule] = {}\n",
    "            all_results[instance]['bestci'][schedule] = {}\n",
    "\n",
    "            # probs = []\n",
    "            probs = {k: [] for k in boots}\n",
    "            time_to_sol = {k: [] for k in boots}\n",
    "            prob_np = {k: [] for k in boots}\n",
    "            ttscs = {k: [] for k in boots}\n",
    "            times = []\n",
    "            b = {k: [] for k in boots}\n",
    "            bnp = {k: [] for k in boots}\n",
    "            bcs = {k: [] for k in boots}\n",
    "            for sweep in sweeps:\n",
    "                # Gather instance names\n",
    "                pickle_name = str(instance) + \"_\" + schedule + \"_\" + str(sweep) + \".p\"\n",
    "                pickle_name = os.path.join(pickle_path, pickle_name)\n",
    "                # If the instance data exists, load the data\n",
    "                if os.path.exists(pickle_name) and not overwrite_pickles:\n",
    "                    samples = pickle.load(open(pickle_name, \"rb\"))\n",
    "                    time_s = samples.info['timing']\n",
    "                # If it does not exist, generate the data\n",
    "                else:\n",
    "                    start = time.time()\n",
    "                    samples = simAnnSampler.sample(\n",
    "                        model_random, num_reads=1000, num_sweeps=sweep, beta_schedule_type=schedule)\n",
    "                    time_s = time.time() - start\n",
    "                    samples.info['timing'] = time_s\n",
    "                    pickle.dump(samples, open(pickle_name, \"wb\"))\n",
    "                # Compute statistics\n",
    "                energies = samples.data_vectors['energy']\n",
    "                occurrences = samples.data_vectors['num_occurrences']\n",
    "                total_counts = sum(occurrences)\n",
    "                times.append(time_s)\n",
    "                if min(energies) < min_energy:\n",
    "                    min_energy = min(energies)\n",
    "                    # print(\"A better solution of \" + str(min_energy) + \" was found for sweep \" + str(sweep))\n",
    "                # success = min_energy*(1.0 + treshold/100.0)**np.sign(min_energy)\n",
    "                success = random_energy - (random_energy - min_energy)*(1.0 - treshold/100.0)\n",
    "                \n",
    "                # Best of boot samples es computed via n_boot bootstrapping\n",
    "                ci=68\n",
    "                boot_dist = {}\n",
    "                pr_dist = {}\n",
    "                cilo = {}\n",
    "                ciup = {}\n",
    "                pr = {}\n",
    "                pr_cilo = {}\n",
    "                pr_ciup = {}\n",
    "                for boot in boots:\n",
    "                    boot_dist[boot] = []\n",
    "                    pr_dist[boot] = []\n",
    "                    for i in range(int(n_boot)):\n",
    "                        resampler = np.random.randint(0, total_reads, boot)\n",
    "                        sample_boot = energies.take(resampler, axis=0)\n",
    "                        # Compute the best along that axis\n",
    "                        boot_dist[boot].append(min(sample_boot))\n",
    "        \n",
    "                        occurences = occurrences.take(resampler, axis=0)\n",
    "                        counts = {}\n",
    "                        for index, energy in enumerate(sample_boot):\n",
    "                            if energy in counts.keys():\n",
    "                                counts[energy] += occurences[index]\n",
    "                            else:\n",
    "                                counts[energy] = occurences[index]\n",
    "                        pr_dist[boot].append(sum(counts[key] for key in counts.keys() if key < success)/boot)\n",
    "                    prob_np[boot] = np.array(pr_dist[boot])\n",
    "                    pr[boot] = np.mean(prob_np[boot])\n",
    "                    probs[boot].append(pr[boot])\n",
    "                    \n",
    "                    b[boot].append(np.mean(boot_dist[boot]))\n",
    "                    # Confidence intervals from bootstrapping the best out of boot\n",
    "                    bnp[boot] = np.array(boot_dist[boot])\n",
    "                    cilo[boot] = np.apply_along_axis(stats.scoreatpercentile, 0, bnp[boot], 50.-ci/2.)\n",
    "                    ciup[boot] = np.apply_along_axis(stats.scoreatpercentile, 0, bnp[boot], 50.+ci/2.)  \n",
    "                    bcs[boot].append((cilo[boot],ciup[boot]))\n",
    "                    # Confidence intervals from bootstrapping the TTS of boot\n",
    "                    if prob_np[boot].all() == 0:\n",
    "                        time_to_sol[boot].append(np.inf)\n",
    "                        ttscs[boot].append((np.inf, np.inf))\n",
    "                    else:\n",
    "                        pr_cilo[boot] = np.apply_along_axis(stats.scoreatpercentile, 0, prob_np[boot], 50.-ci/2.)\n",
    "                        pr_ciup[boot] = np.apply_along_axis(stats.scoreatpercentile, 0, prob_np[boot], 50.+ci/2.)\n",
    "                        time_to_sol[boot].append(time_s*math.log10(1-s)/math.log10(1-pr[boot]+1e-9))   \n",
    "                        ttscs[boot].append((time_s*math.log10(1-s)/math.log10(1-pr_cilo[boot]+1e-9),time_s*math.log10(1-s)/math.log10(1-pr_ciup[boot]+1e-9)))\n",
    "                    \n",
    "\n",
    "            all_results[instance]['t'][schedule][default_sweeps] = times\n",
    "            all_results[instance]['min_energy'][schedule][default_sweeps] = min_energy\n",
    "            all_results[instance]['random_energy'][schedule][default_sweeps] = random_energy\n",
    "            for boot in boots:\n",
    "                all_results[instance]['p'][schedule][boot] = probs[boot]\n",
    "                all_results[instance]['tts'][schedule][boot] = time_to_sol[boot]\n",
    "                all_results[instance]['ttsci'][schedule][boot] = ttscs[boot]\n",
    "                all_results[instance]['best'][schedule][boot] = [(random_energy - energy) / (random_energy - min_energy) for energy in b[boot]]\n",
    "                all_results[instance]['bestci'][schedule][boot] = [tuple((random_energy - element) / (random_energy - min_energy) for element in energy) for energy in bcs[boot]]\n",
    "\n",
    "    # Save results file in case that we are interested in reusing them\n",
    "    pickle.dump(all_results, open(all_results_name, \"wb\"))\n",
    "else: # Just reload processed datafile\n",
    "    all_results = pickle.load(open(all_results_name, \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap(data, n_boot=1000, ci=68):\n",
    "    boot_dist = []\n",
    "    for i in range(int(n_boot)):\n",
    "        resampler = np.random.randint(0, data.shape[0], data.shape[0])\n",
    "        sample = data.take(resampler, axis=0)\n",
    "        # Median ignoring nans instead of mean\n",
    "        boot_dist.append(np.nanmedian(sample, axis=0))\n",
    "    b = np.array(boot_dist)\n",
    "    s1 = np.apply_along_axis(stats.scoreatpercentile, 0, b, 50.-ci/2.)\n",
    "    s2 = np.apply_along_axis(stats.scoreatpercentile, 0, b, 50.+ci/2.)\n",
    "    return (s1,s2)\n",
    "    \n",
    "def tsplotboot(ax, x, data, error_est, **kw):\n",
    "    if x is None:\n",
    "        x = np.arange(data.shape[1])\n",
    "    # Median ignoring nans instead of mean\n",
    "    est = np.nanmedian(data, axis=0)\n",
    "    mask = ~np.isnan(est)\n",
    "    if error_est == 'bootstrap':\n",
    "        cis = bootstrap(data)\n",
    "    elif error_est == 'std':\n",
    "        sd = np.nanstd(data, axis=0)\n",
    "        cis = (est - sd, est + sd)\n",
    "    ax.fill_between(x[mask],cis[0][mask],cis[1][mask],alpha=0.35, **kw)\n",
    "    ax.plot(x[mask],est[mask],**kw)\n",
    "    ax.margins(x=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we bootstrap our solutions with respect to the whole set of instances, or ensemble, and we use the median which respresents the solution better than the mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots()\n",
    "\n",
    "for boot in reversed(boots):\n",
    "    for schedule in schedules:\n",
    "        results_array = np.array([np.array(all_results[i]['tts'][schedule][boot]) for i in range(20)])\n",
    "        min_median_tts = min(np.nanmedian(results_array, axis=0))\n",
    "        tsplotboot(ax1, x=np.asarray(sweeps), data=results_array, error_est='bootstrap', label=\"Ensemble \" + schedule + ' with ' + str(boot) + ' reads')\n",
    "\n",
    "        ax1.plot(sweeps,results['tts'][boot][schedule], label=\"Instance \" + schedule + \"_boot\" + str(boot))\n",
    "\n",
    "\n",
    "ax1.set(yscale='log')\n",
    "\n",
    "ax1.set(ylabel='Time To Solution within '+ str(treshold) +' % of best found [s]')\n",
    "ax1.set(xlabel='Sweeps')\n",
    "plt.title('Simulated annealing expected runtime of \\n' +\n",
    "          ' Ising 42 N=100 with varying schedule and sweeps')\n",
    "\n",
    "plt.legend(loc='upper center', bbox_to_anchor=(0.5, -0.2),\n",
    "          ncol=3, fancybox=False, shadow=False)\n",
    "\n",
    "ax2 = plt.axes([.45, .55, .4, .3])\n",
    "for schedule in schedules:\n",
    "    results_array = np.array([np.array(all_results[i]['tts'][schedule][default_sweeps]) for i in range(20)])\n",
    "    min_median_index = np.argmin(np.nanmedian(results_array, axis=0))\n",
    "    min_median_sweep = sweeps[min_median_index]\n",
    "    min_tts = min(results['tts'][default_sweeps][schedule])\n",
    "    min_index = results['tts'][default_sweeps][schedule].index(min_tts)\n",
    "    min_sweep = sweeps[results['tts'][default_sweeps][schedule].index(min_tts)]\n",
    "    if min_sweep < min_median_sweep:\n",
    "        index_lo = min_index\n",
    "        index_hi = min_median_index\n",
    "    else:\n",
    "        index_lo = min_median_index\n",
    "        index_hi = min_index\n",
    "    print(\"minimum median TTS for \" + schedule + \" schedule = \" + str(min_median_tts) + \"s at sweep = \" + str(min_median_sweep))\n",
    "    ax2.semilogy(sweeps[index_lo-5:index_hi+5],\n",
    "        np.median([all_results[i]['tts'][schedule][default_sweeps] for i in range(20)],axis=0)\n",
    "        [index_lo-5:index_hi+5], '-s')\n",
    "    print(\"minimum TTS for instance 42 with \" + schedule + \" schedule = \" + str(min_tts) + \"s at sweep = \" + str(min_sweep))\n",
    "    ax2.semilogy(sweeps[index_lo-5:index_hi+5], \n",
    "        results['tts'][default_sweeps][schedule][index_lo-5:index_hi+5], '-s')\n",
    "    \n",
    "    \n",
    "ax2.hlines(np.median([all_results[i]['tts'][schedule][default_sweeps][-1] for i in range(20)]),\n",
    "        sweeps[index_lo-5], sweeps[index_hi+5],\n",
    "           linestyle='--', label='default', colors='b')\n",
    "\n",
    "ax2.set(ylabel='TTS [s]')\n",
    "ax2.set(xlabel='Sweeps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "for boot in boots:\n",
    "    for schedule in schedules:\n",
    "        best_array = np.array([np.array(all_results[i]['best'][schedule][boot]) for i in range(20)])\n",
    "        # min_median_best = min(np.nanmedian(results_array, axis=0))\n",
    "        tsplotboot(ax, x=np.asarray(sweeps), data=best_array, error_est='bootstrap', label=\"Ensemble \" + schedule + ' with ' + str(boot) + ' reads')\n",
    "ax.set(xlabel='Sweeps')\n",
    "ax.set(ylabel='Approximation ratio \\n = best found / min energy')\n",
    "ax.set_title('Simulated annealing approximation ratio of \\n' +\n",
    "          ' Ensemble of Ising N=100 with varying schedule, sweeps, and number of reads')\n",
    "plt.legend(ncol=3, loc='upper center', bbox_to_anchor=(0.5, -0.15))\n",
    "ax.set(xscale='log')\n",
    "ax.set(ylim=[0.8,1.01])\n",
    "# ax.set(xlim=[1,200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "for boot in boots:\n",
    "    reads = [s * boot for s in sweeps]\n",
    "    for schedule in schedules:\n",
    "        best_array = np.array([np.array(all_results[i]['best'][schedule][boot]) for i in range(20)])\n",
    "        # min_median_best = min(np.nanmedian(results_array, axis=0))\n",
    "        tsplotboot(ax, x=np.asarray(reads), data=best_array, error_est='bootstrap', label=\"Ensemble \" + schedule + ' with ' + str(boot) + ' reads')\n",
    "ax.set(xlabel='Total number of reads')\n",
    "ax.set(ylabel='Approximation ratio \\n = best found / min energy')\n",
    "ax.set_title('Simulated annealing approximation ratio of \\n' +\n",
    "          ' Ensemble of Ising N=100 with varying schedule, sweeps, and number of reads')\n",
    "plt.legend(ncol=3, loc='upper center', bbox_to_anchor=(0.5, -0.15))\n",
    "ax.set(xscale='log')\n",
    "ax.set(ylim=[0.8,1.01])\n",
    "# ax.set(xlim=[1,200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "instances = np.arange(20)\n",
    "indices = [np.argmin(all_results[i]['tts']['geometric'][default_sweeps]) for i in range(20)]\n",
    "minima = [np.min(all_results[i]['tts']['geometric'][default_sweeps]) for i in range(20)]\n",
    "default = [all_results[i]['tts']['geometric'][default_sweeps][-1] for i in range(20)]\n",
    "median_all = [all_results[i]['tts']['geometric'][default_sweeps][min_median_index] for i in range(20)]\n",
    "\n",
    "\n",
    "ax.bar(instances-0.2, minima, width=0.2, color='b', align='center', label='virtual best')\n",
    "ax.bar(instances, median_all, width=0.2, color='g', align='center', label='median')\n",
    "ax.bar(instances+0.2, default, width=0.2, color='r', align='center', label='default')\n",
    "ax.xaxis.get_major_locator().set_params(integer=True)\n",
    "plt.xlabel('Instance')\n",
    "ax.set(ylabel='Time To Solution within '+ str(treshold) +' % of best found [s]')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how much performance would we be losing if we had used the default value for all these instances, and how much we could eventually win if we knew the best for each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "interest_sweeps = [min_sweep, total_reads, 10, 500]\n",
    "interest_sweeps.append(min_median_sweep)\n",
    "shots = range(1,1000,1)\n",
    "n_boot = 1000\n",
    "\n",
    "# Gather instance names\n",
    "instance = 42\n",
    "for sweep in interest_sweeps:\n",
    "    for schedule in schedules:\n",
    "        if sweep in approx_ratio[schedule] and sweep in approx_ratioci[schedule]:\n",
    "            pass\n",
    "        else:\n",
    "            min_energy = results['min_energy'][schedule]\n",
    "            random_energy = results['random_energy'][schedule]\n",
    "            \n",
    "            pickle_name = str(instance) + \"_\" + schedule + \"_\" + str(sweep) + \".p\"\n",
    "            pickle_name = os.path.join(pickle_path, pickle_name)\n",
    "            # If the instance data exists, load the data\n",
    "            if os.path.exists(pickle_name) and not overwrite_pickles:\n",
    "                # print(pickle_name)\n",
    "                samples = pickle.load(open(pickle_name, \"rb\"))\n",
    "                time_s = samples.info['timing']\n",
    "            # If it does not exist, generate the data\n",
    "            else:\n",
    "                start = time.time()\n",
    "                samples = simAnnSampler.sample(model_random, num_reads=total_reads, num_sweeps=sweep, beta_schedule_type=schedule)\n",
    "                time_s = time.time() - start\n",
    "                samples.info['timing'] = time_s\n",
    "                pickle.dump(samples, open(pickle_name, \"wb\"))\n",
    "            # Compute statistics\n",
    "            energies=samples.data_vectors['energy']\n",
    "            if min(energies) < min_energy:\n",
    "                min_energy = min(energies)\n",
    "                print(\"A better solution of \" + str(min_energy) + \" was found for sweep \" + str(sweep))\n",
    "\n",
    "            b = []\n",
    "            bcs = []\n",
    "            probs = []\n",
    "            time_to_sol = []\n",
    "            for shot in shots:\n",
    "                shot_dist = []\n",
    "                pr_dist = []\n",
    "                for i in range(int(n_boot - shot + 1)):\n",
    "                    resampler = np.random.randint(0, total_reads, shot)\n",
    "                    sample_shot = energies.take(resampler, axis=0)\n",
    "                    # Compute the best along that axis\n",
    "                    shot_dist.append(min(sample_shot))   \n",
    "                \n",
    "                b.append(np.mean(shot_dist))\n",
    "                # Confidence intervals from bootstrapping the best out of shot\n",
    "                bnp = np.array(shot_dist)\n",
    "                cilo = np.apply_along_axis(stats.scoreatpercentile, 0, bnp, 50.-ci/2.)\n",
    "                ciup = np.apply_along_axis(stats.scoreatpercentile, 0, bnp, 50.+ci/2.)  \n",
    "                bcs.append((cilo,ciup))\n",
    "            \n",
    "            approx_ratio[schedule][sweep] = [(random_energy - energy) / (random_energy - min_energy) for energy in b]\n",
    "            approx_ratioci[schedule][sweep] = [tuple((random_energy - element) / (random_energy - min_energy) for element in energy) for energy in bcs]\n",
    "\n",
    "        ax.plot([shot*sweep for shot in shots], approx_ratio[schedule][sweep], label=str(sweep) + ' sweeps')\n",
    "        approx_ratio_bestci_np = np.stack(approx_ratioci[schedule][sweep], axis=0).T\n",
    "        ax.fill_between([shot*sweep for shot in shots],approx_ratio_bestci_np[0],approx_ratio_bestci_np[1],alpha=0.25)\n",
    "ax.set(xscale='log')\n",
    "ax.set(ylim=[0.95,1.001])\n",
    "ax.set(xlim=[1e2,1e4])\n",
    "ax.set(xlabel='Total number of reads (equivalent to time)')\n",
    "ax.set(ylabel='Approximation ratio = \\n ' + '(best found - random sample) / (min energy - random sample)')\n",
    "ax.set_title('Simulated annealing approximation ratio of Ising 42 N=100\\n' +\n",
    "          ' with varying schedule, ' + str(n_boot) + ' bootstrap re-samples, and sweeps')\n",
    "plt.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
